{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMvixwQRjNPwQg8yzLz7OQf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadYaumil1212/ML/blob/master/FinalLatihan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j_ktWMQskR5",
        "colab_type": "code",
        "outputId": "cd1aa522-c408-4145-f8b3-1d5cc8bda95b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# from sklearn.tree import DecissionClassifier\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# data = [[120000, 33], [40000000, 50], [20000, 20]]\n",
        "# scaler = MinMaxScaler()\n",
        "# scaler.fit(data)\n",
        "# print(scaler.transform(data))\n",
        "\n",
        "# from sklearn import preprocessing\n",
        "\n",
        "# data = [[120000, 33], [40000000, 50], [20000, 20]]\n",
        "# scaler = preprocessing.StandardScaler().fit(data)\n",
        "# data = scaler.transform(data)\n",
        "# print(data)\n",
        "\n",
        "# import sklearn \n",
        "# from sklearn import datasets\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# iris = datasets.load_iris()\n",
        "\n",
        "# x = iris.data\n",
        "# y = iris.target\n",
        "\n",
        "# x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.2)\n",
        "\n",
        "# len(x)\n",
        "\n",
        "# import sklearn\n",
        "# from sklearn import datasets\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "# from sklearn import tree\n",
        "\n",
        "# iris = datasets.load_iris()\n",
        "# x = iris.data\n",
        "# y = iris.target\n",
        "\n",
        "# clf = tree.DecisionTreeClassifier()\n",
        "# scores = cross_val_score(clf,x,y, cv=5 )\n",
        "# print(scores)\n",
        "\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# import pandas as pd\n",
        "# from sklearn.datasets import load_iris\n",
        "# iris = pd.read_csv('Iris.csv')\n",
        "# # iris.head()\n",
        "# #menghapus data yang tidak perlu\n",
        "# iris.drop('Id',axis=1,inplace=True)\n",
        "# #Memisahkan antara atribut dan label\n",
        "# X = iris[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm' ]]\n",
        "# y = iris['Species']\n",
        "#  # membuat model Decision Tree\n",
        "# tree_model = DecisionTreeClassifier() \n",
        "# # melakukan pelatihan model terhadap data\n",
        "# tree_model.fit(X, y)\n",
        "# #prediksi model\n",
        "#  # tree_model.predict([[SepalLength, SepalWidth, PetalLength, PetalWidth]])\n",
        "# tree_model.predict([[5.1, 3.5, 1.4, 0.2]])\n",
        "\n",
        "# import numpy as np \n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.linear_model import LinearRegression \n",
        "# #buat data jml kamar\n",
        "# bedrooms = np.array([1,2,3,3,4,4,2,5,6,7,7])\n",
        "# #perkiraan harga kamar\n",
        "# price = np.array([100000,200000,500000,8000000,9000000,200000,900000,8000000,4000000,30000000,200000])\n",
        "# #menampilkan scatter plot \n",
        "# %matplotlib inline\n",
        "# # plt.scatter(bedrooms, price)\n",
        "\n",
        "# bedrooms = bedrooms.reshape(-1,1)\n",
        "# linreg = LinearRegression()\n",
        "# linreg.fit(bedrooms, price)\n",
        "# plt.scatter(bedrooms, price)\n",
        "# plt.plot(bedrooms, linreg.predict(bedrooms))\n",
        "\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn import linear_model\n",
        "\n",
        "# df = pd.read_csv('Social_Network_Ads.csv')\n",
        "# # df.head()\n",
        "# # df.info()\n",
        "# data = df.drop(columns=['User ID'])\n",
        "# data = pd.get_dummies(data)\n",
        "# # print(data)\n",
        "# prediksi = ['Age','EstimatedSalary','Gender_Female','Gender_Male']\n",
        "# X = data[prediksi]\n",
        "# y = data['Purchased']\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "# model = linear_model.LogisticRegression()\n",
        "# #latih Model\n",
        "# model.fit(X_train, y_train)\n",
        "# #Melihat Hasil Latih (Menguji Akurasi)\n",
        "# model.score(X_test, y_test)\n",
        "\n",
        "# import pandas as pd \n",
        "# import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "# import seaborn as sns\n",
        "# from sklearn.cluster import KMeans\n",
        "\n",
        "# #ubah file csv menjadi dataframe\n",
        "# df = pd.read_csv('Social_Network_Ads.csv')\n",
        "# #tampilkan 3 baris pertama\n",
        "# # df.head(3)\n",
        "# #ubah nama kolom data kategorik\n",
        "# df = df.rename(columns={'Gender': 'gender', 'Age': 'age',\n",
        "#                         'Annual Income (k$)': 'annual_income',\n",
        "#                         'Spending Score (1-100)': 'spending_score',\n",
        "#                           'User ID' : 'user_id' })\n",
        "\n",
        "# #ubah data kategorik menjadi numerik\n",
        "# df['gender'].replace(['Female','Male'], [0,1], inplace=True)\n",
        "\n",
        "# # tampilkan data yang sudah di preprocess\n",
        "# df.head(3)\n",
        "# #menghilangkan kolom customer id gender\n",
        "# X = df.drop(['user_id', 'gender'], axis=1)\n",
        "# #membuat List yang berisi inersia\n",
        "# clusters = []\n",
        "# for i in range(1,11):\n",
        "#   #membuat objek Kmeans\n",
        "#   km = KMeans(n_clusters=i).fit(X)\n",
        "\n",
        "#   clusters.append(km.inertia_)\n",
        "# #membuat plot inersia\n",
        "# fig, ax = plt.subplots(figsize=(8,4))\n",
        "# sns.lineplot(x=list(range(1,11)), y=clusters, ax=ax)\n",
        "# ax.set_title('cari Elbow')\n",
        "# ax.set_xlabel('clusters')\n",
        "# ax.set_ylabel('inertia')\n",
        "\n",
        "# km5 = KMeans(n_clusters=5).fit(X)\n",
        "# #menambahkan kolom label pada dataset\n",
        "# X['Labels'] = km5.labels_\n",
        "# #membuat plot KMeans dengan 5 klaster\n",
        "# plt.figure(figsize=(8,4))\n",
        "# sns.scatterplot(X['EstimatedSalary'], X['Purchased'], hue=X['Labels'],\n",
        "#                 palette=sns.color_palette('hls', 5))\n",
        "# plt.title('KMeans dengan 5 Cluster')\n",
        "# plt.show()\n",
        "\n",
        "# from sklearn.decomposition import PCA\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn import datasets\n",
        "# from sklearn import tree\n",
        "\n",
        "# iris = datasets.load_iris()\n",
        "# atribut = iris.data\n",
        "# label = iris.target\n",
        "# #bagi dataset menjadi train set dan test set\n",
        "# X_train,X_test,y_train,y_test = train_test_split(atribut, label, test_size=0.2)\n",
        "\n",
        "# decision_tree = tree.DecisionTreeClassifier()\n",
        "# model_pertama = decision_tree.fit(X_train, y_train)\n",
        "# model_pertama.score(X_test, y_test)\n",
        "\n",
        "# from sklearn.decomposition import PCA\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn import datasets\n",
        "# from sklearn import tree\n",
        "\n",
        "# #membuat objek PCA dengan 4 principal component\n",
        "\n",
        "# pca = PCA(n_components=4)\n",
        "# #mengaplikasikan PCA pada dataset\n",
        "# pca_attributes = pca.fit_transform(X_train)\n",
        "# #melihat variance dari setiap atribut\n",
        "# pca.explained_variance_ratio_\n",
        "\n",
        "# pca = PCA(n_components=2)\n",
        "# X_train_pca = pca.fit_transform(X_train)\n",
        "# X_test_pca = pca.fit_transform(X_test)\n",
        "\n",
        "# model2 = decision_tree.fit(X_train_pca,y_train)\n",
        "# model2.score(X_test_pca,y_test)\n",
        "\n",
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.svm import SVC\n",
        "\n",
        "# df = pd.read_csv('diabetes.csv')\n",
        "# # df.head()\n",
        "# # df.info()\n",
        "\n",
        "# #memisahkan atribut dataset dan menyimpannya pada sebuah variabel\n",
        "# X = df[df.columns[:8]]\n",
        "\n",
        "# #Memisahkan label pada dataset ~\n",
        "\n",
        "# y = df['Outcome']\n",
        "\n",
        "# #standarisasi nilai-nilai dari dataset\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# scaler.fit(X)\n",
        "# X = scaler.transform(X)\n",
        "\n",
        "# X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.3, random_state=42)\n",
        "\n",
        "# clf = SVC()\n",
        "# clf.fit(X_train,y_train)\n",
        "# clf.score(X_test,y_test)\n",
        "\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# import numpy as np\n",
        "# from sklearn.svm import SVR\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# data = pd.read_csv('Salary_Data.csv')\n",
        "# data.head()\n",
        "\n",
        "# X = data['YearsExperience']\n",
        "# y = data['Salary']\n",
        "# X = X[:,np.newaxis]\n",
        "\n",
        "# model = SVR(C=1000, gamma=0.05, kernel='rbf')\n",
        "# model.fit(X,y)\n",
        "\n",
        "# model = SVR()\n",
        "# parameters = {\n",
        "#     'kernel': ['rbf'],\n",
        "#     'C': [1000,1000,100000],\n",
        "#     'gamma': [0.5,0.05,0.005]\n",
        "# }\n",
        "# grid_search = GridSearchCV(model, parameters)\n",
        "# grid_search.fit(X,y)\n",
        "# print(grid_search.best_params_)\n",
        "# model_baru = SVR(C=100000, gamma=0.005, kernel='rbf')\n",
        "# model_baru.fit(X,y)\n",
        "# plt.scatter(X,y)\n",
        "# plt.plot(X, model_baru.predict(X))\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# print(tf.__version__)\n",
        "\n",
        "# !wget --no-check-certificate \\\n",
        "#  https://dicodingacademy.blob.core.windows.net/picodiploma/ml_pemula_academy/messy-vs-clean-room.zip \\\n",
        "#  -O /tmp/messy_vs_clean_room.zip\n",
        "\n",
        "#mengekstrasi data\n",
        "import zipfile,os\n",
        "local_zip = '/tmp/messy_vs_clean_room.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "base_dir = '/tmp/images'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'val')\n",
        "\n",
        "os.listdir('/tmp/images/train')\n",
        "os.listdir('/tmp/images/val')\n",
        "\n",
        "#membuat direktori ruangan rapi pada direktori data training\n",
        "train_clean_dir = os.path.join(train_dir, 'clean')\n",
        "#membuat direktori ruangan berantakan pada direktori data training\n",
        "train_messy_dir = os.path.join(train_dir, 'messy')\n",
        "#membuat ruangan rapi pada direktori data validasi\n",
        "validation_clean_dir = os.path.join(validation_dir, 'clean')\n",
        "#membuat ruangan berantakan pada direktori data validasi\n",
        "validation_clean_dir = os.path.join(validation_dir, 'messy')\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    horizontal_flip=True,\n",
        "    shear_range=0.2,\n",
        "    fill_mode = 'nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    horizontal_flip=True,\n",
        "    shear_range=0.2,\n",
        "    fill_mode = 'nearest')\n",
        "\n",
        "#mempersiapkan data latih yang akan dipelajari model\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir, #direktori datalatih\n",
        "    target_size=(150,150),\n",
        "    batch_size=4,\n",
        "# karena kita merupakan masalah klasifikasi 2 kelas maka menggunakan class_mode = 'binary'\n",
        "     class_mode='binary'\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150,150),\n",
        "    batch_size=4,\n",
        "    class_mode='binary')\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.optimizers.Adam(),\n",
        "              metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=25, #berapa batch yang akan dieksekusi pada setiap epoch\n",
        "    epochs=20,\n",
        "    validation_data=validation_generator,# menampilkan akurasi pengujian data validasi\n",
        "    validation_steps=5, #berapa batch yang akan dieksekusi pada setiap epoch\n",
        "    verbose=2\n",
        ")\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  #prediksi image\n",
        "  path = fn\n",
        "  img = image.load_img(path, target_size=(150,150))\n",
        "  imgplot = plt.imshow(img)\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "\n",
        "  print(fn)\n",
        "\n",
        "  if classes==0:\n",
        "    print('clean')\n",
        "  else:\n",
        "    print('messy')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 192 images belonging to 2 classes.\n",
            "Found 20 images belonging to 2 classes.\n",
            "Epoch 1/20\n",
            "25/25 - 6s - loss: 0.7021 - accuracy: 0.5000 - val_loss: 0.6928 - val_accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "25/25 - 6s - loss: 0.6972 - accuracy: 0.4500 - val_loss: 0.6899 - val_accuracy: 0.7000\n",
            "Epoch 3/20\n",
            "25/25 - 6s - loss: 0.6878 - accuracy: 0.5300 - val_loss: 0.6890 - val_accuracy: 0.5000\n",
            "Epoch 4/20\n",
            "25/25 - 6s - loss: 0.6965 - accuracy: 0.4200 - val_loss: 0.6813 - val_accuracy: 0.5500\n",
            "Epoch 5/20\n",
            "25/25 - 6s - loss: 0.6839 - accuracy: 0.7000 - val_loss: 0.6953 - val_accuracy: 0.5000\n",
            "Epoch 6/20\n",
            "25/25 - 6s - loss: 0.6928 - accuracy: 0.5700 - val_loss: 0.6941 - val_accuracy: 0.5000\n",
            "Epoch 7/20\n",
            "25/25 - 6s - loss: 0.6934 - accuracy: 0.4600 - val_loss: 0.6675 - val_accuracy: 0.5500\n",
            "Epoch 8/20\n",
            "25/25 - 6s - loss: 0.6518 - accuracy: 0.6400 - val_loss: 0.6173 - val_accuracy: 0.6500\n",
            "Epoch 9/20\n",
            "25/25 - 6s - loss: 0.6475 - accuracy: 0.7300 - val_loss: 0.5162 - val_accuracy: 0.7000\n",
            "Epoch 10/20\n",
            "25/25 - 6s - loss: 0.5792 - accuracy: 0.6800 - val_loss: 0.5050 - val_accuracy: 0.8500\n",
            "Epoch 11/20\n",
            "25/25 - 6s - loss: 0.6142 - accuracy: 0.7300 - val_loss: 0.6173 - val_accuracy: 0.5500\n",
            "Epoch 12/20\n",
            "25/25 - 6s - loss: 0.6224 - accuracy: 0.6200 - val_loss: 0.5808 - val_accuracy: 0.7000\n",
            "Epoch 13/20\n",
            "25/25 - 6s - loss: 0.4924 - accuracy: 0.7700 - val_loss: 0.9283 - val_accuracy: 0.6500\n",
            "Epoch 14/20\n",
            "25/25 - 6s - loss: 0.6335 - accuracy: 0.6900 - val_loss: 0.5584 - val_accuracy: 0.7500\n",
            "Epoch 15/20\n",
            "25/25 - 6s - loss: 0.5483 - accuracy: 0.7400 - val_loss: 0.4460 - val_accuracy: 0.8000\n",
            "Epoch 16/20\n",
            "25/25 - 6s - loss: 0.5448 - accuracy: 0.7800 - val_loss: 0.5765 - val_accuracy: 0.6500\n",
            "Epoch 17/20\n",
            "25/25 - 6s - loss: 0.5174 - accuracy: 0.7200 - val_loss: 0.4097 - val_accuracy: 0.9000\n",
            "Epoch 18/20\n",
            "25/25 - 6s - loss: 0.4651 - accuracy: 0.8100 - val_loss: 0.3942 - val_accuracy: 0.8500\n",
            "Epoch 19/20\n",
            "25/25 - 6s - loss: 0.5053 - accuracy: 0.7200 - val_loss: 0.3944 - val_accuracy: 0.8500\n",
            "Epoch 20/20\n",
            "25/25 - 6s - loss: 0.5580 - accuracy: 0.7400 - val_loss: 0.4266 - val_accuracy: 0.8000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-66c6e2c3-0d48-498e-8c9c-4161bc3ad7c0\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-66c6e2c3-0d48-498e-8c9c-4161bc3ad7c0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-64f2fdcf05b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m   \u001b[0;31m#prediksi image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m   result = _output.eval_js(\n\u001b[1;32m     63\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m---> 64\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Cannot read property '_uploadFiles' of undefined"
          ]
        }
      ]
    }
  ]
}